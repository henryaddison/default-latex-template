\section{Downscaling} \label{sec:downscalingbg}

The starting point for climate projections are GCMs, which simulate the physical processes of the atmosphere for the entire globe. They get used to generate projections spanning centuries. The computational cost of such models require them to divide the Earth into a very coarse grid (typically resolutions of 25-300km). Such resolution means they cannot capture the influence of regional or local-scale attributes nor represent local phenomenon that are important to decision makers. For example, \textcite{scahller2020resolutionrole} present an example where a change in temporal and spatial resolution of modelled rainfall has a large impact on where flooding is predicted to occur. Therefore it is common to downscale these global models to smaller grids and this is often done in one of two ways: dynamical regional climate models or statistical methods.

\subsection{Dynamical downscaling} \label{sec:downscalingbg:dynamical}

Dynamical downscaling uses finer-resolution simulations, RCMs. Like GCMs, RCMs use numerical approximations to dynamically simulate physical processes affecting the climate but by using GCMs to create boundary conditions they need only cover a sub-global region so may more feasibly use a finer horizontal grid \cite{tapiador2020rcmreview}. Effectively an RCM is a higher resolution simulation embedded inside a coarser, global one.

Even at these higher resolutions, it is not possible to simulate all relevant processes. These processes are instead estimated using a parameterization scheme. These are heuristics that approximate the effects of these processes on the variables of the model. Clouds are such an example. Their small size does not allow them to be resolved in a grid box of a GCM but since their effects are significant their contribution is approximated based on variables available to the model \cite{rasp2018dlsubgridclimmodel}. The rough approximation of the parameterization scheme is one source of error in a climate model.

One symptom of errors in a climate model is the presence of systematic bias in the projections it generates. Bias can be observed in the output of an RCM for a period with observations. They may have, for example, temperatures that are too hot on average or predict too much rainfall on average compared to the observed records \cite{christensen2008rcmbiascorrection}. This can either be accepted as a known problem or attempts at correcting it can be made by adjusting the output to match observations (though this assumes that bias in the model remains the same of outputs outside of periods with observations) \cite{tapiador2020rcmreview}. This latter approach of adjusting the output is known as bias correction.

Typically RCMs downscale to resolutions of around 10-50km but finer-grained simulations are possible and semi-tractable (the computational costs are still very large). One particular such class of finer-grained RCMs are Convection Permitting Models (CPMs).

\subsubsection{Convection Permitting Models} \label{sec:downscalingbg:cpm}

CPMs are dynamical downscalers which use a fine enough resolution that they can also model convective processes rather than \textquote{represent[ing] the average effects of on the atmosphere of convection} \cite{kendon2019ukcpscience} with a parameterization scheme.

In a first for national climate scenarios, in 2019 the UK's Met Office released 2.2km projections \cite{kendon2019ukcpscience}. The UKCP18 Local Projections at 2.2km resolution use a CPM. The finer resolution of a CPM allows more detailed representation of the atmospheric effects of convection. This allows better ability to represent convective storms and more \textquote{credible estimates of changes in convectively-driven phenomena, such as hourly rainfall extremes} \cite{kendon2019ukcpscience}. The trade-off for the high computational cost is a reduction in the ensemble size, time-horizon and emission scenarios explored compared to the Met Office's coarser projections.

In practice this means that only 12 for the 15 available climate models for a single emission scenario were downscaled to 2.2km using the CPM \cite{kendon2019ukcpscience}. Emission scenarios, called Representative Concentration Pathways (RCPs), cover assumptions about changes to GHG emissions in the future and their corresponding increase in radiative forcing (i.e. the climate is made hotter due to reduced radiative output from the atmosphere caused by GHG retaining heat in the atmosphere). The emission scenario used by all the CPM projections is RCP8.5 (8.5 because it corresponds to a radiative forcing of \(8.5\textrm{Wm}^{-2}\) by 2100). It is the most extreme scenario that the Met Office covers in UKCP18 in which emissions are assumed to continue rising without any efforts to reduce them \cite{metoffice2018rcpguidance} and is considered by some to be \textquote{implausible} and \textquote{misleading} \cite{hausfather2020rcp85misleading}. Further, rather the contiguous time period 100-year period from 1981 to 2080 used by the 12km RCM, only three 20-year chunks are covered: 1981-2000, 2021-2040, 2061-2080.

\subsection{Statistical downscaling} \label{sec:downscalingbg:statistical}

Statistical downscaling attempts to fit a statistical relationship between a lower-resolution set of climate variables (predictors) from a simulation and higher-resolution values (predictands), often observations but for this project projections of a CPM. They are much less computationally expensive than dynamical downscaling methods but it is uncertain how well they work for future climate conditions \cite{maraun2019sddownscaling}.

Statistical downscaling techniques do not directly model the underlying physics. This means, unlike RCMs, they may be unable to capture emergent behaviour in the climate and extrapolate well to new conditions or to take advantage of patterns not realistically represented due to the coarse resolution of predictors \cite{maraun2019sddownscaling}. This is particularly important for future projections because the climate is expected to change but the downscaled output cannot be compared and adjusted based on the observation record. However, they do offer advantages in that they may be more easily applied to different GCMs and are computationally cheaper and the problem of an assumed static climate is potentially mitigated in my project by using conditioning inputs from simulations of the future that will include climate change signals.

Many approaches have been tried based on both traditional statistics and machine learning. \textcite{gutierrez2019sdcomparison} assess the skill of 45 different methods for precipitation alone for a single European collaborative experiment, though the well-performing methods relevant to this project's problem are mainly based on generalized linear models (GLMs) or nearest neighbours (methods of analogs). I will use a GLM approach as a baseline (see Section \ref{sec:emulatorstudy:baselines}). Some approaches fit a separate relationship for each grid box while others fit a model for the entire grid. No method has proven better than the others in general \cite{gutierrez2019sdcomparison, vandal2018mldownscaling, fowler2007downscaling4hydrology}. \textcite{vandal2018mldownscaling} found no clear best option when comparing traditional approaches with some off-the-shelf machine learning approaches (Multi-task Sparse Structure Learning and Autoencoder Neural Networks) for daily and extreme precipitation over the Northeastern United States. \textcite{maraun2019sddownscaling} note a lack of work on spatial dependence. The machine learning approaches tested by \textcite{vandal2018mldownscaling} are deterministic in the sense that once trained, the same conditioning GCM input will produce the high-resolution output. Without a probabilistic element, these models struggle to predict the small-scale detail of precipitation \cite{ravuri2021deepgenprecip}. I plan to curate more carefully a more recent probabilistic deep learning architecture to solve these problems with statistical downscaling.

\subsubsection{Learning from projections rather than observations}

Comparisons of different statistical downscaling approaches such as that carried out by VALUE have so far been mainly based on matching observations rather than emulating models to match their projections \cite{maraun2019sddownscaling}. This limits the available data and makes no guarantees that such approaches will continue to work as the climate changes in the future. It also reduces the number of locations that require predictions for each time step with only 86 carefully chosen observation stations compared to nearly 300,000, closely-packed squares in the full 2.2km grid required to cover the whole of the UK and Ireland.

\textcite{walton2015hybriddownscaling} carry out similar work using statistical methods to more quickly downscale a set of GCMs to 2km based on the dynamical downscaling of a separate set of GCMs. Their work focuses on LA and considers just the long-term mean warming based on the downscaling projections across the full set of GCMs. I plan to predict precipitation time series at daily and then sub-daily resolutions.

Works such as those by \textcite{gentine2018mlsuperparam} or \textcite{rasp2018dlsubgridclimmodel} also use projection datasets rather than observations to combine dynamical and ML-based statistical downscaling. The neural nets developed, CBRAIN \cite{gentine2018mlsuperparam} and NNCAM \cite{rasp2018dlsubgridclimmodel} respectively, are trained on high-resolution dynamical model data aimed at resolving clouds and intended to provide a replacement parameterization of subgrid process inside a GCM that is skilful enough to rival the dynamic approach they replace but fast enough to use to cover the whole globe. As with the work by \textcite{vandal2018mldownscaling} these neural nets are deterministic and the intention to run them inside a GCM adds technical costs and constraints to their usage. They also only cover one part of the parameterization scheme rather than the entire model and do not produce higher resolution outputs.
\todo[inline]{ask Peter about his comment "the parameterisation emulation work is rather different - could explain this more clearly"}

The scope of this project is on precipitation in the UK and Ireland. In particular, I will emulate the CPM used by Met Office to make projections at 2.2km resolution. I used the Met Office's UKCP18 datasets for both the GCM global 60km projections \cite{ukcp18global} and the CPM local 2.2km projections \cite{ukcp18local}. The Met Office's CPM 2.2km projections are the first of their kind at a national level \cite{kendon2019ukcpscience} so while other coarse global projections exist using different GCMs, they have no large, local-scale CPM output associated with them. The next section explores more advanced machine learning approaches that have worked for a similar problem, natural image super-resolution.
